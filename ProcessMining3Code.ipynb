{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd906ba9",
   "metadata": {},
   "source": [
    "# Introduction to process discovery (Directly Follow Graph)\n",
    "\n",
    "By: Jakub Kot, Piotr Kubala, Rafał Łukosz, Piotr Karaś, Tomasz Kawiak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05503e28",
   "metadata": {},
   "source": [
    "Today's lab builds upon the codes developed in the previous lab.\n",
    "\n",
    "## 1. [10 points] Automatic CSV Column Matching\n",
    "\n",
    "Using ipywidgets, extend your existing interface from the previous lab by adding the ability to upload a CSV file with event data. Your solution should attempt to automatically detect and match the columns corresponding to case ID, activity, and timestamp. If automatic matching fails, the user should be prompted to manually select the appropriate columns (e.g., using dropdown lists).\n",
    "\n",
    "When implementing the matching logic, consider:\n",
    "\n",
    "- Partial matches in column names,\n",
    "\n",
    "- The data types in each column,\n",
    "\n",
    "- Rules or heuristics that should apply for each element (case ID, activity, timestamp).\n",
    "\n",
    "Define and document:\n",
    "\n",
    "- The key features used for column detection,\n",
    "\n",
    "- The method of assessing the best match, especially in logs without headers.\n",
    "\n",
    "After automatic or manual matching, the user should be able to confirm or adjust the selected columns such as case_id, activity_key, and timestamp_key from the available subset of columns in the log.\n",
    "\n",
    "\n",
    "## 2. [5 points] Testing\n",
    "\n",
    "Test your solution on various logs and include in your report a summary table describing the matching results.\n",
    "For each log, provide:\n",
    "\n",
    "- The name of the log,\n",
    "\n",
    "- The total number of columns,\n",
    "\n",
    "- Which fields were detected,\n",
    "all fields were detected correctly\n",
    "\n",
    "- Whether user intervention was required,\n",
    "no user intervention, however we might have overfitted our assumptions a lil bit :3\n",
    "\n",
    "\n",
    "- Comments on any detection issues.\n",
    "sepsis.csv has no start timestamp, so we decided to use the end timestamp as both start and end timestamps.\n",
    "\n",
    "we have some wild assumptions like the first column is case id (if it's not, we ask the user to select it manually), but we provide our own propositions\n",
    "\n",
    "same thingy with activity column - we assume it's the second column (if it's not, ask for confirmation)\n",
    "\n",
    "there's that one csv with no headers so we had to detect if headers are present in the csv, and if no then create generic column names for it\n",
    "\n",
    "\n",
    "Use the logs available in the “Sample logs” folder on MS Teams.\n",
    "Test your solution on both the previously used logs (logExample, purchasingExample, repairExample, sepsis) with and without headers, and on the newly added logs with non-standard or missing labels:\n",
    "\n",
    "new_example_changed_labels.csv, new_teleclaims_changed_labels.csv, new_reviewing_no_column_names.csv.\n",
    "\n",
    "\n",
    "## 3. [15 points] Improving the Filtering Method\n",
    "\n",
    "Based on the observations from previous labs — particularly regarding threshold selection and issues with model filtering — refine your filtering method to improve model quality.\n",
    "Test the new functionality and evaluate whether the filtered models show better results.\n",
    "\n",
    "In your report, describe at least three improvements, each with:\n",
    "\n",
    "- The filtering thresholds used,\n",
    "\n",
    "- An example diagram (or a part of it) before and after the improvement,\n",
    "\n",
    "- A brief explanation of the change and its effect,\n",
    "\n",
    "- A code excerpt showing the implementation.\n",
    "\n",
    "If your previous reports missed certain observations or requirements, consider addressing some of the following ones:\n",
    "\n",
    "- Ensuring no task becomes detached from the model (each task should remain connected).\n",
    "\n",
    "- Ensuring all tasks are part of a path from process start to process end.\n",
    "\n",
    "- Minimizing the number of flows.\n",
    "\n",
    "- Maximizing the total flow frequency.\n",
    "\n",
    "\n",
    "Note:\n",
    "This week, I will collect, summarize, and publish the observations and problems reported in your previous reports.\n",
    "During the next class (18.11), there will be no new report — instead, we will test your solutions using various logs and thresholds. Therefore, at least one person from each group must attend the next class in person (in classroom C2 316, on-site attendance).\n",
    "Remember to display after each filtering the process model, the number of edges (flows), and the total sum of labels in the remaining flows. \n",
    "In this challenge, we will compare the results, and the winning groups demonstrating the most robust tools will receive bonus points for the labs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "43f29c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from itertools import pairwise\n",
    "from collections import Counter\n",
    "import pygraphviz as pgv\n",
    "from IPython.display import Image, display\n",
    "import ipywidgets as widgets\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = [\"logs/logExample.csv\",\n",
    "        \"logs/new_example_changed_labels.csv\",\n",
    "        \"logs/new_reviewing_no_column_names.csv\",\n",
    "        \"logs/new_teleclaims_changed_labels.csv\", \n",
    "        \"logs/purchasingExample.csv\",\n",
    "        \"logs/repairExample.csv\",\n",
    "        \"logs/sepsis.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b2d7c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_header(file_path, bytes = 8192):\n",
    "    sniffer = csv.Sniffer()\n",
    "    data = open(file_path, \"r\").read(bytes)\n",
    "    has_header = sniffer.has_header(data)\n",
    "    return has_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d90f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_column_names(csv_file: str) -> tuple[pd.DataFrame, str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Retrieve column names for start date, end date, case ID, and activity from a CSV log file.\n",
    "    How does it work:\n",
    "    1. Load the CSV file into a DataFrame.\n",
    "    2. \n",
    "    3. Done\n",
    "\n",
    "    Returns:\n",
    "    Dataframe, Start date, End date, Case ID, Activity column names\n",
    "    If a column name was not found, None is returned in its place.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # check if the first row of csv properly contains column names\n",
    "    has_header = check_header(csv_file)\n",
    "\n",
    "    df = pd.read_csv(csv_file, header=0 if has_header else None)  # Load your log file here\n",
    "\n",
    "    if not has_header:\n",
    "        # assign generic column names\n",
    "        df.columns = [f\"col_{i}\" for i in range(len(df.columns))]\n",
    "\n",
    "    # convert Start Date to datetime\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            # check if column can be converted to int (if yes, skip)\n",
    "            try:\n",
    "                _ = df[column].dropna().astype(int)\n",
    "                continue\n",
    "            except:\n",
    "                df[column] = pd.to_datetime(df[column], format=\"mixed\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "\n",
    "    if len(datetime_columns) == 2:\n",
    "        # do first column minus second column, if it's negative then first column is start date\n",
    "        if ((df[datetime_columns[0]] - df[datetime_columns[1]]).dt.total_seconds() > 0).sum() > ((df[datetime_columns[0]] - df[datetime_columns[1]]).dt.total_seconds() < 0).sum():\n",
    "\n",
    "            start_date_column = datetime_columns[1]\n",
    "            end_date_column = datetime_columns[0]\n",
    "        else:\n",
    "            start_date_column = datetime_columns[0]\n",
    "            end_date_column = datetime_columns[1]\n",
    "\n",
    "    else:\n",
    "        start_date_keywords = ['start', 'begin', 'open', 'created', 'initiated']\n",
    "        end_date_keywords = ['end', 'close', 'completed', 'finished', 'complete', 'resolved']\n",
    "        datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "        start_date_column = None\n",
    "        for col in datetime_columns:\n",
    "            if any(keyword in col.lower() for keyword in start_date_keywords):\n",
    "                start_date_column = col\n",
    "                break\n",
    "\n",
    "        end_date_column = None\n",
    "        for col in datetime_columns:\n",
    "            if any(keyword in col.lower() for keyword in end_date_keywords):\n",
    "                end_date_column = col\n",
    "                break\n",
    "\n",
    "        if len(datetime_columns) == 1:\n",
    "            if start_date_column is None and end_date_column is not None:\n",
    "                start_date_column = end_date_column\n",
    "            elif end_date_column is None and start_date_column is not None:\n",
    "                end_date_column = start_date_column\n",
    "\n",
    "        if start_date_column is None and end_date_column is None:\n",
    "            # MANUAL!!\n",
    "            ...\n",
    "\n",
    "    case_id_column = None\n",
    "\n",
    "    # check amount of rows\n",
    "    if len(df) < 200:\n",
    "        # MANUAL\n",
    "        ...\n",
    "    elif any(any(keyword in col.lower() for keyword in ['case id', 'case_id']) for col in df.columns if col not in datetime_columns):\n",
    "        # find the column that contains the keyword\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in ['case id', 'case_id']):\n",
    "                case_id_column = col\n",
    "                break\n",
    "    else:\n",
    "        case_id_column = sorted(\n",
    "            [(i, col, df[col].nunique() if col not in datetime_columns else 0) for i, col in enumerate([col for col in df.columns if col not in datetime_columns])],\n",
    "            key=lambda x: (x[2], -x[0])\n",
    "        )[-1][1]\n",
    "        if case_id_column != 0:\n",
    "            # MANUAL\n",
    "            ...\n",
    "\n",
    "    potential_columns = set(datetime_columns)\n",
    "    potential_columns.add(case_id_column)\n",
    "\n",
    "    # for each column count (except case id and datetime) in how many unique case IDs it appears\n",
    "    activity_columns = [col for col in df.columns if col not in potential_columns]\n",
    "    # compute scoring per column (sum of distinct values per case as before) and produce ranking\n",
    "    if case_id_column is not None:\n",
    "        activity_scores = {col: df.groupby(case_id_column)[col].nunique().sum() for col in activity_columns}\n",
    "    else:\n",
    "        activity_scores = {col: df[col].nunique() for col in activity_columns}\n",
    "    activity_rank = pd.Series(activity_scores).sort_values(ascending=False)\n",
    "\n",
    "    # keep the top candidate for backward compatibility\n",
    "    activity_column = activity_rank.index[0]\n",
    "\n",
    "    return df, start_date_column, end_date_column, case_id_column, activity_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f61c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: logs/logExample.csv\n",
      "Total columns: 9\n",
      "  Start Date: Start Date\n",
      "  End Date: End Date\n",
      "  Case ID: Case ID\n",
      "  Activity: Activity\n",
      "\n",
      "Processing file: logs/new_example_changed_labels.csv\n",
      "Total columns: 9\n",
      "  Start Date: When Start\n",
      "  End Date: When End\n",
      "  Case ID: Instance\n",
      "  Activity: Task\n",
      "\n",
      "Processing file: logs/new_reviewing_no_column_names.csv\n",
      "Total columns: 15\n",
      "  Start Date: col_3\n",
      "  End Date: col_4\n",
      "  Case ID: col_0\n",
      "  Activity: col_1\n",
      "\n",
      "Processing file: logs/new_teleclaims_changed_labels.csv\n",
      "Total columns: 13\n",
      "  Start Date: from\n",
      "  End Date: to\n",
      "  Case ID: id\n",
      "  Activity: action\n",
      "\n",
      "Processing file: logs/purchasingExample.csv\n",
      "Total columns: 6\n",
      "  Start Date: Start Timestamp\n",
      "  End Date: Complete Timestamp\n",
      "  Case ID: Case ID\n",
      "  Activity: Activity\n",
      "\n",
      "Processing file: logs/repairExample.csv\n",
      "Total columns: 17\n",
      "  Start Date: Start Timestamp\n",
      "  End Date: Complete Timestamp\n",
      "  Case ID: Case ID\n",
      "  Activity: Activity\n",
      "\n",
      "Processing file: logs/sepsis.csv\n",
      "Total columns: 34\n",
      "  Start Date: Complete Timestamp\n",
      "  End Date: Complete Timestamp\n",
      "  Case ID: Case ID\n",
      "  Activity: Activity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for csv_file in csvs:\n",
    "    print(f\"Processing file: {csv_file}\")\n",
    "    df, start_date_col, end_date_col, case_id_col, activity_col = retrieve_column_names(csv_file)\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"  Start Date: {start_date_col}\")\n",
    "    print(f\"  End Date: {end_date_col}\")\n",
    "    print(f\"  Case ID: {case_id_col}\")\n",
    "    print(f\"  Activity: {activity_col}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7791895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity          5447\n",
      "Resource          5279\n",
      "Agent Position    4118\n",
      "Customer ID       3885\n",
      "Product           3885\n",
      "Service Type      3885\n",
      "dtype: int64\n",
      "Chosen activity_column (top): Activity\n"
     ]
    }
   ],
   "source": [
    "csv_name = \"logs/new_teleclaims_changed_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "69202e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label_value(val, mode):\n",
    "  if mode == 'coverage':\n",
    "    return f\"{val:.1f}%\"\n",
    "  else:\n",
    "    try:\n",
    "      return str(int(val))\n",
    "    except Exception:\n",
    "      return str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4e3548e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = \"logs/logExample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "61598100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/tmp/some-file/ipykernel_846357/3580236922.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['start'] = pd.to_datetime(df['Start Date'])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(example_name)\n",
    "df['start'] = pd.to_datetime(df['Start Date'])\n",
    "dfs = df[['Case ID', 'Activity', 'start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0cdab51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# totals and per-case counts\n",
    "num_cases = dfs['Case ID'].nunique()\n",
    "# absolute occurrences (total events)\n",
    "ev_absolute = dfs['Activity'].value_counts()\n",
    "# per-case activity counts: dataframe with one row per (case, activity)\n",
    "case_activity_counts = dfs.groupby(['Case ID', 'Activity']).size().reset_index(name='count')\n",
    "# case frequency: number of distinct cases containing the activity\n",
    "ev_case_freq = case_activity_counts.groupby('Activity')['Case ID'].nunique()\n",
    "# max repetitions: max occurrences of activity within a single case\n",
    "ev_max_reps = case_activity_counts.groupby('Activity')['count'].max()\n",
    "# case coverage: percentage of cases containing the activity\n",
    "ev_case_coverage = (ev_case_freq / num_cases) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7bf267af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity\n",
       "Call Outbound      377\n",
       "Email Outbound     380\n",
       "Handle Case        532\n",
       "Handle Email       428\n",
       "Inbound Call      3342\n",
       "Inbound Email      388\n",
       "Name: Case ID, dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_case_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_metric_series(mode):\n",
    "  if mode == 'absolute':\n",
    "    return ev_absolute\n",
    "  if mode == 'case':\n",
    "    return ev_case_freq\n",
    "  if mode == 'max':\n",
    "    return ev_max_reps\n",
    "  if mode == 'coverage':\n",
    "    return ev_case_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b402e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_traces = (dfs.sort_values(by=['Case ID', 'start']).groupby(['Case ID']).agg({'Activity': lambda x: tuple(x)}))\n",
    "# build per-case pair counters\n",
    "per_case_pairs = {}\n",
    "for case_id, row in dfs_traces.iterrows():\n",
    "  trace = row['Activity']\n",
    "  pairs = list(pairwise(trace))\n",
    "  per_case_pairs[case_id] = Counter(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0c900ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate flow metrics\n",
    "flow_abs = Counter()\n",
    "flow_case_freq = Counter()\n",
    "flow_max = Counter()\n",
    "for case_id, cnt in per_case_pairs.items():\n",
    "  for pair, c in cnt.items():\n",
    "    flow_abs[pair] += c\n",
    "    if c > 0:\n",
    "      flow_case_freq[pair] += 1\n",
    "    flow_max[pair] = max(flow_max.get(pair, 0), c)\n",
    "\n",
    "flow_case_coverage = {pair: (flow_case_freq[pair] / num_cases) * 100.0 for pair in flow_abs.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c20cd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_metric_dict(mode):\n",
    "  if mode == 'absolute':\n",
    "    return dict(flow_abs)\n",
    "  if mode == 'case':\n",
    "    return dict(flow_case_freq)\n",
    "  if mode == 'max':\n",
    "    return dict(flow_max)\n",
    "  if mode == 'coverage':\n",
    "    return dict(flow_case_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7688d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect start/end events\n",
    "ev_start_set = set()\n",
    "ev_end_set = set()\n",
    "for case_id, row in dfs_traces.iterrows():\n",
    "  trace = row['Activity']\n",
    "  if len(trace) == 0:\n",
    "    continue\n",
    "  ev_start_set.add(trace[0])\n",
    "  ev_end_set.add(trace[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9ff8e",
   "metadata": {},
   "source": [
    "Probably half of this function could be pruned, but I guess it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3aa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_graphs(mode, filter_by='none', thr_ev=0.0, thr_flow=0.0):\n",
    "  \"\"\"Render both simple and weighted DFG images using the chosen mode and optional thresholds.\n",
    "\n",
    "  mode: one of 'absolute','case','max','coverage'\n",
    "  filter_by: 'none','events','flows','both'\n",
    "  thr_ev / thr_flow: numeric thresholds (for coverage use percent 0-100)\n",
    "  \"\"\"\n",
    "  event_metric = get_event_metric_series(mode)\n",
    "  flow_metric = get_flow_metric_dict(mode)\n",
    "\n",
    "  # build DFG mapping using chosen flow metric\n",
    "  dfg = {}\n",
    "  for (a, b), val in flow_metric.items():\n",
    "    if a not in dfg:\n",
    "      dfg[a] = Counter()\n",
    "    dfg[a][b] = val\n",
    "\n",
    "  # determine color bounds from event_metric\n",
    "  event_metric_values = [v for v in (event_metric.reindex(index=dfs['Activity'].unique(), fill_value=0).to_dict().values())]\n",
    "  if len(event_metric_values) > 0:\n",
    "    color_min = min(event_metric_values)\n",
    "    color_max = max(event_metric_values)\n",
    "  else:\n",
    "    color_min = 0\n",
    "    color_max = 1\n",
    "\n",
    "  filter_events = filter_by in ('events', 'both')\n",
    "  filter_flows = filter_by in ('flows', 'both')\n",
    "\n",
    "  # Weighted visualization with nodes and edge labels\n",
    "  trace_counts = sorted(chain(*[c.values() for c in dfg.values()])) if len(dfg) > 0 else [0, 1]\n",
    "  trace_min = trace_counts[0]\n",
    "  trace_max = trace_counts[-1]\n",
    "\n",
    "  Gw = pgv.AGraph(strict=False, directed=True)\n",
    "  Gw.graph_attr['rankdir'] = 'LR'\n",
    "  Gw.node_attr['shape'] = 'Mrecord'\n",
    "\n",
    "  Gw.add_node(\"start\", shape=\"circle\", label=\"\")\n",
    "  for ev_start in sorted(ev_start_set):\n",
    "    if filter_events and event_metric.get(ev_start, 0) < thr_ev:\n",
    "      continue\n",
    "\n",
    "    # find how many traces start with this event\n",
    "    edge_label = sum(1 for _, row in dfs_traces.iterrows() if row['Activity'] and row['Activity'][0] == ev_start)\n",
    "\n",
    "    Gw.add_edge(\"start\", ev_start, penwidth=3.5, label=edge_label, style=\"dotted\")\n",
    "\n",
    "  for event, succesors in dfg.items():\n",
    "    ev_val = event_metric.get(event, 0)\n",
    "    if filter_events and ev_val < thr_ev:\n",
    "      continue\n",
    "\n",
    "    # color scale; avoid division by zero\n",
    "    if color_max != color_min:\n",
    "      color = int(float(color_min - ev_val) / float(color_min - color_max) * 100.00)\n",
    "    else:\n",
    "      color = 5\n",
    "    \n",
    "    my_color = \"#ff9933\" + str(hex(max(0, min(255, color)))[2:])\n",
    "    label_val = format_label_value(ev_val, mode)\n",
    "    Gw.add_node(event, style=\"rounded,filled\", fillcolor=my_color, label=f\"{event} ({label_val})\")\n",
    "    \n",
    "    for succesor, cnt in succesors.items():\n",
    "      if filter_flows and cnt < thr_flow:\n",
    "        continue\n",
    "      if filter_events and event_metric.get(succesor, 0) < thr_ev:\n",
    "        continue\n",
    "      \n",
    "      lbl = format_label_value(cnt, mode)\n",
    "      pen = 4 * float(cnt) / (trace_max - trace_min + 1) + 0.1\n",
    "      Gw.add_edge(event, succesor, penwidth=min(pen, 5.0), label=lbl)\n",
    "\n",
    "  Gw.add_node(\"end\", shape=\"circle\", label=\"\", penwidth='3')\n",
    "  for ev_end in sorted(ev_end_set):\n",
    "    if filter_events and event_metric.get(ev_end, 0) < thr_ev:\n",
    "      continue\n",
    "    ev_val = event_metric.get(ev_end, 0)\n",
    "\n",
    "    # find how many traces end with this event\n",
    "    edge_label = sum(1 for _, row in dfs_traces.iterrows() if row['Activity'] and row['Activity'][-1] == ev_end)\n",
    "\n",
    "    if color_max != color_min:\n",
    "      color = int(float(color_min - ev_val) / float(color_min - color_max) * 100.00)\n",
    "    else:\n",
    "      color = 0\n",
    "    my_color = \"#ff9933\" + str(hex(max(0, min(255, color)))[2:])\n",
    "    label_val = format_label_value(ev_val, mode)\n",
    "    Gw.add_node(ev_end, style=\"rounded,filled\", fillcolor=my_color, label=f\"{ev_end} ({label_val})\")\n",
    "    Gw.add_edge(ev_end, \"end\", penwidth=3.5, label=f\"{edge_label}\", style=\"dotted\")\n",
    "\n",
    "  Gw.draw('simple_heuristic_net_with_events.png', prog='dot')\n",
    "  display(Image('simple_heuristic_net_with_events.png'))\n",
    "\n",
    "  # summary\n",
    "  total_edges = Gw.number_of_edges()\n",
    "  sum_edge_labels = 0\n",
    "  for e in Gw.edges():\n",
    "    try:\n",
    "      lab = e.attr['label']\n",
    "      if lab is None:\n",
    "        continue\n",
    "      if isinstance(lab, str) and lab.endswith('%'):\n",
    "        sum_edge_labels += float(lab.rstrip('%'))\n",
    "      else:\n",
    "        sum_edge_labels += float(lab)\n",
    "    except Exception:\n",
    "      continue\n",
    "  print(f\"Mode: {mode}. Filter: {filter_by}. Thr-events: {thr_ev}. Thr-flows: {thr_flow}. Number of edges: {total_edges}, Label sum: {sum_edge_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7fbecfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"absolute\"  # Default mode\n",
    "filter_by = 'none'  # Default filter\n",
    "threshold_events = 0.0  # Default threshold for events\n",
    "threshold_flows = 0.0  # Default threshold for flows\n",
    "\n",
    "mode_dropdown = widgets.Dropdown(options=['absolute', 'case', 'max', 'coverage'], value=mode, description='Mode:')\n",
    "filter_dropdown = widgets.Dropdown(options=['none', 'events', 'flows', 'both'], value=filter_by, description='Filter:')\n",
    "\n",
    "event_metric = get_event_metric_series(mode)\n",
    "\n",
    "try:\n",
    "  max_ev = int(event_metric.max())\n",
    "except Exception:\n",
    "  max_ev = 100 if mode == 'coverage' else 1\n",
    "flow_metric = get_flow_metric_dict(mode)\n",
    "try:\n",
    "  max_flow = int(max(flow_metric.values())) if len(flow_metric) > 0 else 1\n",
    "except Exception:\n",
    "  max_flow = 1\n",
    "# coverage mode expects percent thresholds\n",
    "if mode == 'coverage':\n",
    "  max_ev = 100\n",
    "  max_flow = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c62bb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_ev_box = widgets.IntSlider(value=min(int(threshold_events), max_ev), min=0, max=max_ev if max_ev>0 else 1, step=1, description='Thr events:')\n",
    "thr_flow_box = widgets.IntSlider(value=min(int(threshold_flows), max_flow), min=0, max=max_flow if max_flow>0 else 1, step=1, description='Thr flows:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5ba153f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _on_mode_change(change):\n",
    "  if change.get('name') != 'value':\n",
    "    return\n",
    "  new_mode = change.get('new')\n",
    "  ev_m = get_event_metric_series(new_mode)\n",
    "  try:\n",
    "    new_max_ev = int(ev_m.max())\n",
    "  except Exception:\n",
    "    new_max_ev = 100 if new_mode == 'coverage' else 1\n",
    "  flow_m = get_flow_metric_dict(new_mode)\n",
    "  try:\n",
    "    new_max_flow = int(max(flow_m.values())) if len(flow_m) > 0 else 1\n",
    "  except Exception:\n",
    "    new_max_flow = 1\n",
    "  if new_mode == 'coverage':\n",
    "    new_max_ev = 100\n",
    "    new_max_flow = 100\n",
    "  # set slider maxima and clamp values if needed\n",
    "  thr_ev_box.max = new_max_ev if new_max_ev > 0 else 1\n",
    "  thr_flow_box.max = new_max_flow if new_max_flow > 0 else 1\n",
    "  if thr_ev_box.value > thr_ev_box.max:\n",
    "    thr_ev_box.value = thr_ev_box.max\n",
    "  if thr_flow_box.value > thr_flow_box.max:\n",
    "    thr_flow_box.value = thr_flow_box.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "57593bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_dropdown.observe(_on_mode_change, names='value')\n",
    "generate_btn = widgets.Button(description='Generate graph')\n",
    "out = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0a262505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _on_generate_clicked(b):\n",
    "  # clear previous output (graphs) but keep widgets visible\n",
    "  out.clear_output(wait=True)\n",
    "  mode_w = mode_dropdown.value\n",
    "  filter_w = filter_dropdown.value\n",
    "  thr_ev_w = float(thr_ev_box.value)\n",
    "  thr_flow_w = float(thr_flow_box.value)\n",
    "  # remove old graph files if present to avoid stale images\n",
    "  for fn in ('simple_heuristic_net.png', 'simple_heuristic_net_with_events.png'):\n",
    "    try:\n",
    "      if os.path.exists(fn):\n",
    "        os.remove(fn)\n",
    "    except Exception:\n",
    "      pass\n",
    "  with out:\n",
    "    render_graphs(mode_w, filter_w, thr_ev_w, thr_flow_w)\n",
    "\n",
    "generate_btn.on_click(_on_generate_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a4a871df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03aa2164f6c64cf3891ebedf28f7e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Mode:', options=('absolute', 'case', 'max', 'coverage'), value='absolute'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a6e2f6c9534fd6a0d008266771f595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ui = widgets.HBox([mode_dropdown, filter_dropdown, thr_ev_box, thr_flow_box, generate_btn])\n",
    "display(ui)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dbf3d",
   "metadata": {},
   "source": [
    "bfs - znajdź dystans do końca dla każdego node (odwrócone krawędzie), zapisuj dystans do końca dla każdego node\n",
    "\n",
    "start 0 - zakolejkuj wszystkie node\n",
    "dla każdego z node weź max(wchodzące, których dystans do końca jest większy niż twój dystans do końca) + on sam  (przechowuj poprzednika)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kluza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
